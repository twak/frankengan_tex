




\section{Notes (to be deleted later)}

GANs for facade synthesis seem promising~\cite{pix2pix} because:
\begin{itemize}
    \item creating textures is time consuming and expensive for an artist.
    \item every texture in an environment can be unique, as in the real world.
    \item creating textures is currently an art. GANS + data can make it a science. data-driven texturing can be achieved by non-skilled people...
    \item ...disentangling style and structure allows us to texture known structures.
    \item at some point it takes less space to to store the network rather than the textures.
\end{itemize}

But using them in the real world is difficult to use because:
\begin{itemize}
    \item style is difficult to define, being multi-dimensional and largely driven by subjective opinion. Previous works can separate style from structure~\cite{papers} but not control distribution of style, such as artists require when creating a range of objects. 
    \item navigating style-space is hard. It is difficult to control distribution to ensure variety (see picture of many-different-sized-by similarly-textured buildings). 
    \item Similarly we want to control the appearance distribution of sub-features such as windows. "with this distribution of facade, use this distribution of windows"
    \item high resolution uses lots of memory~\cite{pix2pixHD}
    \item we often want to intervene and apply fine-grained control: "move this window 3cm to the left", "add a balcony here".
    \item GANs have problems creating consistent larger-scale features that aren't present in the conditional channel. (Pix2pix window results). Procedural models are good at creating such globally synchronized structures (e.g. windows on a floor having the same height). Simple repeating textures can create better results than GAN when, for example, creating a repeating brick pattern on a wall (GANs find it hard to keep the bricks level).
    \item we wish to use multiple networks, but need them to cooperate; for example the pixel-colours where 2 walls meet should be the same. Or, different sides of a building must share the same style. Or, similar windows need to be instanced at each location over a single facade.
    \item we wish to combine layers of procedurally generated, GAN generated, and user generated content. The order in which these are used depends on the artist's needs.
\end{itemize}

To solve these problems we introduce the Franken-GAN facade texturing system. We contribute:

\begin{itemize}
    \item A new type of image-to-image translation GAN which is able to generate draw samples from a user-defined distribution of styles.
    \item A set of GANs which fit on a single 8Gb card, which are able to generate photo-realistic textures for facades, windows, doors, and roofs of urban models. 
    \item The use of procedural models to regularize the conditional structure input into these GANs. We can use urban priors to synchronize, i.e. the tops of windows.
    \item Several sets of training data we created to train the networks to texture urban environments.
    \item An interactive system demonstrating just-in-time texture generation in a virtual environment. (possibly not on a single 8gb card...)
    \item A demonstration of the interactive texturing of a large scale city scene, in which every one of the 10,000 textures (32Gb when JPG compressed) is unique.
\end{itemize}

\vspace{1in}

{\bf GAN formulations.}
In technical terms, GAN is a generative model consisting of a Discriminator $\mathbf{D}_{X}$ and a Generator $\mathbf{G}$ that are adversarially trained in a similar manner as a minimax game. The discriminator tries to correctly determine if a given image is real (from training) or fake (generated by $\mathbf{G}$). The Generator on the other hand is trying to change the generated samples to better fool $\mathbf{D}_{X}$. This can be formulated as the following minimax game on the loss function $\mathbf{\mathit{L}}_{GAN}(\mathbf{G},\mathbf{D}_{X},\mathbf{P}_{X})$: 
\begin{equation}
\begin{aligned}
&
\min_{\mathbf{G}} \max_{\mathbf{D}_{X}}~~\mathbf{\mathit{L}}_{\text{GAN}}(\mathbf{G},\mathbf{D}_{X},\mathbf{P}_{X})=  
& \mathbb{E}_{\mathbf{x}\sim p_{x}(\mathbf{x})} [\log \mathbf{D}_{X}(x)] +  \mathbb{E}_{\mathbf{z}\sim p_{\mathbf{z}}(\mathbf{z})} [\log (1- \mathbf{D}_{X}(\mathbf{G}(z)))]
\end{aligned}
\label{eq:GAN}
% \vspace{-5pt}
\end{equation}
% \vspace{-5pt}
\noindent where $\mathbf{P}_{X}$ is the distribution of images in domain X and $ \mathbf{z} \in \mathbb{R}^{d}$ is the latent uniformly distributed random vector.