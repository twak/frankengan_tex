\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces We introduce \textsc  {FrankenGAN}\xspace  , a method to add detail to coarse mass models. This method allows the user to automatically generate diverse geometry and texture detail for a mass model (blue), while also giving the user control over the resulting style (through exemplar images, red). Geometric details (green) and textures (right) are generated by multiple generative adversarial networks with synchronized styles. A detailed view of this model is shown in Figure\nonbreakingspace \ref  {fig:london}.\relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:teaser}{{1}{1}{We introduce \systemName , a method to add detail to coarse mass models. This method allows the user to automatically generate diverse geometry and texture detail for a mass model (blue), while also giving the user control over the resulting style (through exemplar images, red). Geometric details (green) and textures (right) are generated by multiple generative adversarial networks with synchronized styles. A detailed view of this model is shown in Figure~\ref {fig:london}.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Wonka:2003:IA,Mueller:2006:PMB,Schwarz:2015:APM}
\citation{Bokeloh:2012:AMP,Lin:2011:SPR,Bao:2013:PFV,Ilcik:2015:LBP,Dang:2014:SAF}
\citation{Vanegas:2012:IDU}
\citation{Talton:2011:MPM,Yeh:2013:STP,Ritchie:2015:CPM}
\citation{Teboul:PAMI:2013}
\citation{Mathias:IJCV:2016,Cohen:CVPR:2014}
\citation{Stolcke:1994:IPG}
\citation{Talton:2012:LDP,Martinovic:2013:BGL}
\citation{Nishida:2016:ISU:2897824.2925951}
\citation{Wang:2018:DCP}
\citation{he2016deep,krizhevsky2012imagenet}
\citation{long2015fully}
\citation{escorcia2016daps,tran2015learning}
\citation{GAN}
\citation{WGAN,BEGAN,LaplacianPyrmid,Bigan,inception-GANs2,catGAN,improvGANfeature,zhao2016energy}
\citation{pix2pix}
\citation{cycleGAN}
\citation{zhu2017multimodal}
\citation{inpainting}
\citation{style-transfer}
\citation{pix2pix,superresolution}
\citation{manifold-manipulation}
\citation{hand-pose}
\citation{face-recog}
\citation{StreetGAN}
\citation{Berger:TVCG:2018}
\citation{sceneGAN}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Naive options to texture a mass model give unconvincing results. Pix2Pix shows heavy mode collapse, discussed in Section\nonbreakingspace \ref  {sec:qualitative_comparisons}.\relax }}{2}{figure.caption.3}}
\newlabel{fig:intro_naive}{{2}{2}{Naive options to texture a mass model give unconvincing results. Pix2Pix shows heavy mode collapse, discussed in Section~\ref {sec:qualitative_comparisons}.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}}
\citation{pix2pix}
\citation{Karras:2018:PGG}
\citation{zhu2017multimodal}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces In contrast to photogrammetric reconstruction (second column), our method can be used to synthesize new facade layouts and textures. Style and variation can be controlled by the user; in columns 3 to 5, we show details generated by \textsc  {FrankenGAN}\xspace  with low to high style variation.\relax }}{3}{figure.caption.4}}
\newlabel{fig:photogrammetric}{{3}{3}{In contrast to photogrammetric reconstruction (second column), our method can be used to synthesize new facade layouts and textures. Style and variation can be controlled by the user; in columns 3 to 5, we show details generated by \systemName with low to high style variation.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Overview}{3}{section.3}}
\newlabel{sec:overview}{{3}{3}{Overview}{section.3}{}}
\citation{pix2pix}
\citation{zhu2017multimodal}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textsc  {FrankenGAN}\xspace  overview. Individual GANs are denoted by G, and yellow rectangles denote GAN chains or geometry generation modules. Given a mass model and an optional style reference (any part can be replaced by a random style), the model is detailed in three steps. First, two chains of GANs generate texture and label maps for facades and roofs. Then, the resolution of the generated textures is increased by a dedicated window generation chain and two super-resolution GANs for roofs and facades. Finally, 3D details are generated for roofs, windows, and facades based on the generated textures and label maps.\relax }}{4}{figure.caption.7}}
\newlabel{fig:overview}{{4}{4}{\systemName overview. Individual GANs are denoted by G, and yellow rectangles denote GAN chains or geometry generation modules. Given a mass model and an optional style reference (any part can be replaced by a random style), the model is detailed in three steps. First, two chains of GANs generate texture and label maps for facades and roofs. Then, the resolution of the generated textures is increased by a dedicated window generation chain and two super-resolution GANs for roofs and facades. Finally, 3D details are generated for roofs, windows, and facades based on the generated textures and label maps.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}GAN architecture}{4}{section.4}}
\newlabel{sec:gan_architecture}{{4}{4}{GAN architecture}{section.4}{}}
\newlabel{eq:gan_function}{{1}{4}{GAN architecture}{equation.4.1}{}}
\newlabel{eq:loss_gan_d}{{2}{4}{GAN architecture}{equation.4.2}{}}
\newlabel{eq:loss_gan_g}{{3}{4}{GAN architecture}{equation.4.3}{}}
\citation{pix2pix}
\citation{Donahue:2016:afl,Dumoulin:2016:ali,vae_gan}
\citation{vae}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces GAN architecture. The setup used during evaluation is shown in the top row, and the training setup is shown in the bottom row. Dotted lines denote random sampling.\relax }}{5}{figure.caption.8}}
\newlabel{fig:gan}{{5}{5}{GAN architecture. The setup used during evaluation is shown in the top row, and the training setup is shown in the bottom row. Dotted lines denote random sampling.\relax }{figure.caption.8}{}}
\newlabel{eq:loss_gan_l1}{{4}{5}{GAN architecture}{equation.4.4}{}}
\newlabel{eq:loss_gan_kl}{{5}{5}{GAN architecture}{equation.4.5}{}}
\newlabel{eq:loss_gan_lr}{{6}{5}{GAN architecture}{equation.4.6}{}}
\newlabel{eq:loss_gan}{{7}{5}{GAN architecture}{equation.4.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Additional input channels. GANs are conditioned on additional channels that include information about the global context at each pixel. Given a facade/roof mask, we include the distance to the facade boundary and the distance to each bounding box side, making it easy for the network to decide how far it is from the boundary and at what height on the facade.\relax }}{5}{figure.caption.9}}
\newlabel{fig:context_info}{{6}{5}{Additional input channels. GANs are conditioned on additional channels that include information about the global context at each pixel. Given a facade/roof mask, we include the distance to the facade boundary and the distance to each bounding box side, making it easy for the network to decide how far it is from the boundary and at what height on the facade.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}FrankenGAN}{5}{section.5}}
\newlabel{sec:franken_gan}{{5}{5}{FrankenGAN}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textsc  {FrankenGAN}\xspace  details. Each GAN chain (yellow rectangles) consists of several GANs (G) that each perform an image-to-image transformation. GANs are usually conditioned on additional inputs (arrows along the bottom) and are guided by a reference style (arrows along the top). Label outputs are regularized (R) to obtain clean label rectangles. Figure\nonbreakingspace \ref  {fig:overview} shows these chains in context.\relax }}{6}{figure.caption.10}}
\newlabel{fig:franken_gan}{{7}{6}{\systemName details. Each GAN chain (yellow rectangles) consists of several GANs (G) that each perform an image-to-image transformation. GANs are usually conditioned on additional inputs (arrows along the bottom) and are guided by a reference style (arrows along the top). Label outputs are regularized (R) to obtain clean label rectangles. Figure~\ref {fig:overview} shows these chains in context.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Style Control}{6}{subsection.5.1}}
\newlabel{sec:style_control}{{5.1}{6}{Style Control}{subsection.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Specifying a style distribution gives control over the generated details. The top row shows three results generated with the same user-specified style distribution, while the bottom row uses the style prior, giving random styles. Note how the buildings in the top row have a consistent style while still allowing for some variation (depending on the variance chosen by the user), while the bottom row does not have a consistent style.\relax }}{6}{figure.caption.11}}
\newlabel{fig:specified_vs_random_style}{{8}{6}{Specifying a style distribution gives control over the generated details. The top row shows three results generated with the same user-specified style distribution, while the bottom row uses the style prior, giving random styles. Note how the buildings in the top row have a consistent style while still allowing for some variation (depending on the variance chosen by the user), while the bottom row does not have a consistent style.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Detail Generation}{6}{subsection.5.2}}
\newlabel{sec:detail_generation}{{5.2}{6}{Detail Generation}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Roofs}{6}{section*.12}}
\citation{pix2pix}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Super-resolution. Given inputs (green, magenta), the super-resolution network creates high-quality textures for the walls, while the window GAN chain provides high-quality windows. Note that the window label and window texture networks each run once for every window.\relax }}{7}{figure.caption.13}}
\newlabel{fig:super}{{9}{7}{Super-resolution. Given inputs (green, magenta), the super-resolution network creates high-quality textures for the walls, while the window GAN chain provides high-quality windows. Note that the window label and window texture networks each run once for every window.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Facades}{7}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{Windows}{7}{section*.15}}
\@writefile{toc}{\contentsline {paragraph}{Super-resolution}{7}{section*.16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Regularizers}{7}{subsection.5.3}}
\newlabel{sec:regularizers}{{5.3}{7}{Regularizers}{subsection.5.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Roof detail labels}{7}{section*.18}}
\@writefile{toc}{\contentsline {paragraph}{Facade window and door labels}{7}{section*.19}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \leavevmode {\color  [rgb]{0,0,0}Regularization. All our label maps are regularized before being used by the next GAN in the chain. Rows: roof details, facade windows, facade details, and window details (regularized with an outer product). }\relax }}{8}{figure.caption.17}}
\newlabel{fig:regularizers}{{10}{8}{\changed {Regularization. All our label maps are regularized before being used by the next GAN in the chain. Rows: roof details, facade windows, facade details, and window details (regularized with an outer product). }\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {paragraph}{Facade detail labels}{8}{section*.20}}
\@writefile{toc}{\contentsline {paragraph}{Window detail labels}{8}{section*.21}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Generated window textures and labels are merged back into the facade texture, increasing fidelity of window textures, normals and materials.\relax }}{8}{figure.caption.22}}
\newlabel{fig:merge_maps}{{11}{8}{Generated window textures and labels are merged back into the facade texture, increasing fidelity of window textures, normals and materials.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Geometry Synthesis}{8}{subsection.5.4}}
\newlabel{sec:geometry_synthesis}{{5.4}{8}{Geometry Synthesis}{subsection.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}User interface}{8}{subsection.5.5}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{8}{section.6}}
\newlabel{sec:results}{{6}{8}{Results}{section.6}{}}
\citation{cmp_dataset}
\citation{adam}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The distribution designer UI (see supplemental video). Given a style distribution (a), the system continuously shows evaluations of that distribution (c). By clicking on an image, the user can see the network inputs (b). Different networks can be selected (d). The style distribution for any network is a Gaussian mixture model that may have multiple modes (e), the mean of which is given by an exemplar image.\relax }}{9}{figure.caption.23}}
\newlabel{fig:ui}{{12}{9}{The distribution designer UI (see supplemental video). Given a style distribution (a), the system continuously shows evaluations of that distribution (c). By clicking on an image, the user can see the network inputs (b). Different networks can be selected (d). The style distribution for any network is a Gaussian mixture model that may have multiple modes (e), the mean of which is given by an exemplar image.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Datasets used to train our GANs. We use four datasets of labeled images, a few examples are shown here.\relax }}{9}{figure.caption.24}}
\newlabel{fig:dataset}{{13}{9}{Datasets used to train our GANs. We use four datasets of labeled images, a few examples are shown here.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Datasets and Training Setup}{9}{subsection.6.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces GAN statistics: the size of the training data (n), resolution (in pixels squared), number of epochs trained, and if the network takes style as input.\relax }}{9}{table.caption.26}}
\newlabel{table:net_statistics}{{1}{9}{GAN statistics: the size of the training data (n), resolution (in pixels squared), number of epochs trained, and if the network takes style as input.\relax }{table.caption.26}{}}
\citation{Kelly:SIGA:2017}
\citation{Karras:2018:PGG}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Detailed London area. The output of our method is shown on top, and the input style images at the bottom left. This is followed, from left to right, by close-ups of the input mass models, detail geometry generated by our method, and two detailed views of the generated model using super-resolution textures.\relax }}{10}{figure.caption.25}}
\newlabel{fig:london}{{14}{10}{Detailed London area. The output of our method is shown on top, and the input style images at the bottom left. This is followed, from left to right, by close-ups of the input mass models, detail geometry generated by our method, and two detailed views of the generated model using super-resolution textures.\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Qualitative Results}{10}{subsection.6.2}}
\citation{pix2pix}
\citation{zhu2017multimodal}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Detailed Madrid area. Input style images and mass models are shown on the left, an overview of our output in the center, and close-ups of our output and the generated geometry (green), showing details like balconies and window moldings, on the right. Lower right panel uses super-resolution textures.\relax }}{11}{figure.caption.27}}
\newlabel{fig:madrid}{{15}{11}{Detailed Madrid area. Input style images and mass models are shown on the left, an overview of our output in the center, and close-ups of our output and the generated geometry (green), showing details like balconies and window moldings, on the right. Lower right panel uses super-resolution textures.\relax }{figure.caption.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Statistics for the London and Madrid scenes. We show the number of roofs, facades and windows in each block, as well as the time taken to generate the block.\relax }}{11}{table.caption.28}}
\newlabel{table:scene_statistics}{{2}{11}{Statistics for the London and Madrid scenes. We show the number of roofs, facades and windows in each block, as well as the time taken to generate the block.\relax }{table.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Qualitative Comparisons}{11}{subsection.6.3}}
\newlabel{sec:qualitative_comparisons}{{6.3}{11}{Qualitative Comparisons}{subsection.6.3}{}}
\citation{pix2pix}
\citation{zhu2017multimodal}
\citation{pix2pix}
\citation{zhu2017multimodal}
\citation{pix2pix}
\citation{zhu2017multimodal}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Different types of style distributions. From left to right, a constant style is used for all houses, a style chosen randomly from the style prior, a style sampled independently for all building properties, and a dependently sampled property style. Note that in the last two columns, there are several separate modes for properties, such as wall and building color, that are either mixed randomly for each building (third column) or sampled dependently (last column).\relax }}{12}{figure.caption.29}}
\newlabel{fig:eval_style_control}{{16}{12}{Different types of style distributions. From left to right, a constant style is used for all houses, a style chosen randomly from the style prior, a style sampled independently for all building properties, and a dependently sampled property style. Note that in the last two columns, there are several separate modes for properties, such as wall and building color, that are either mixed randomly for each building (third column) or sampled dependently (last column).\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Different super-resolution styles. The original low-resolution facade and roof textures are shown on the left; the middle and right buildings show two different super-resolutions styles, resulting in different textures for the roof tiles or stone wall.\relax }}{12}{figure.caption.30}}
\newlabel{fig:super_style}{{17}{12}{Different super-resolution styles. The original low-resolution facade and roof textures are shown on the left; the middle and right buildings show two different super-resolutions styles, resulting in different textures for the roof tiles or stone wall.\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}\leavevmode {\color  [rgb]{0,0,0}Perceptual} Studies}{12}{subsection.6.4}}
\citation{Nishida:2016:ISU:2897824.2925951}
\citation{Peng:2016:CND}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Qualitative comparison with end-to-end GANs. The left column shows results of Pix2Pix trained to transform empty facade and roof masks to textures. The middle column shows BicycleGAN trained similarly, while the last column shows our method. Note how Pix2Pix suffers from mode collapse, while BicyleGAN has less realistic window layouts and lacks scale and style consistency. \textsc  {FrankenGAN}\xspace  provides better style control and our approach of splitting up the problem into multiple steps opens up several avenues to increase the realism of our models.\relax }}{13}{figure.caption.31}}
\newlabel{fig:qual_comparison}{{18}{13}{Qualitative comparison with end-to-end GANs. The left column shows results of Pix2Pix trained to transform empty facade and roof masks to textures. The middle column shows BicycleGAN trained similarly, while the last column shows our method. Note how Pix2Pix suffers from mode collapse, while BicyleGAN has less realistic window layouts and lacks scale and style consistency. \systemName provides better style control and our approach of splitting up the problem into multiple steps opens up several avenues to increase the realism of our models.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Limitations}{13}{subsection.6.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Perceptual studies comparing our method with/without style guidance (left) and to Pix2Pix\nonbreakingspace \citep  {pix2pix} and BicycleGAN\nonbreakingspace \citep  {zhu2017multimodal} (middle). The average probability for each method of being judged more similar to the reference or more realistic by the study participants is shown on the right. The black bars are $95\%$ confidence intervals.\relax }}{13}{figure.caption.32}}
\newlabel{fig:study}{{19}{13}{Perceptual studies comparing our method with/without style guidance (left) and to Pix2Pix~\cite {pix2pix} and BicycleGAN~\cite {zhu2017multimodal} (middle). The average probability for each method of being judged more similar to the reference or more realistic by the study participants is shown on the right. The black bars are $95\%$ confidence intervals.\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{13}{section.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Limitations. Green: the system is prone to generating green vegetation over missing windows. Red: texture discontinuities at boundaries.\relax }}{13}{figure.caption.33}}
\newlabel{fig:limitations}{{20}{13}{Limitations. Green: the system is prone to generating green vegetation over missing windows. Red: texture discontinuities at boundaries.\relax }{figure.caption.33}{}}
\bibstyle{ACM-Reference-Format}
\bibdata{styleSynchronized,BibtexPeterWonkaMyPaper,gan}
\bibcite{WGAN}{{1}{2017}{{Arjovsky et~al\unhbox \voidb@x \hbox {.}}}{{Arjovsky, Chintala, and Bottou}}}
\bibcite{Bao:2013:PFV}{{2}{2013}{{Bao et~al\unhbox \voidb@x \hbox {.}}}{{Bao, Schwarz, and Wonka}}}
\bibcite{Berger:TVCG:2018}{{3}{2018}{{Berger et~al\unhbox \voidb@x \hbox {.}}}{{Berger, Li, and Levine}}}
\bibcite{BEGAN}{{4}{2017}{{Berthelot et~al\unhbox \voidb@x \hbox {.}}}{{Berthelot, Schumm, and Metz}}}
\bibcite{Bokeloh:2012:AMP}{{5}{2012}{{Bokeloh et~al\unhbox \voidb@x \hbox {.}}}{{Bokeloh, Wand, Seidel, and Koltun}}}
\bibcite{Cohen:CVPR:2014}{{6}{2014}{{Cohen et~al\unhbox \voidb@x \hbox {.}}}{{Cohen, Schwing, and Pollefeys}}}
\bibcite{Dang:2014:SAF}{{7}{2014}{{Dang et~al\unhbox \voidb@x \hbox {.}}}{{Dang, Ceylan, Neubert, and Pauly}}}
\bibcite{LaplacianPyrmid}{{8}{2015}{{Denton et~al\unhbox \voidb@x \hbox {.}}}{{Denton, Chintala, Szlam, and Fergus}}}
\bibcite{Donahue:2016:afl}{{9}{2016}{{Donahue et~al\unhbox \voidb@x \hbox {.}}}{{Donahue, Kr\"ahenb\"uhl, and Darrell}}}
\bibcite{Dumoulin:2016:ali}{{10}{2016}{{Dumoulin et~al\unhbox \voidb@x \hbox {.}}}{{Dumoulin, Belghazi, Poole, Lamb, Arjovsky, Mastropietro, and Courville}}}
\bibcite{escorcia2016daps}{{11}{2016}{{Escorcia et~al\unhbox \voidb@x \hbox {.}}}{{Escorcia, Heilbron, Niebles, and Ghanem}}}
\bibcite{GAN}{{12}{2014}{{Goodfellow et~al\unhbox \voidb@x \hbox {.}}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio}}}
\bibcite{Bigan}{{13}{2017}{{Gurumurthy et~al\unhbox \voidb@x \hbox {.}}}{{Gurumurthy, Kiran~Sarvadevabhatla, and Venkatesh~Babu}}}
\bibcite{StreetGAN}{{14}{2017}{{Hartmann et~al\unhbox \voidb@x \hbox {.}}}{{Hartmann, Weinmann, Wessel, and Klein}}}
\bibcite{he2016deep}{{15}{2016}{{He et~al\unhbox \voidb@x \hbox {.}}}{{He, Zhang, Ren, and Sun}}}
\bibcite{Ilcik:2015:LBP}{{16}{2015}{{Il{\v {c}}{\'{\i }}k et~al\unhbox \voidb@x \hbox {.}}}{{Il{\v {c}}{\'{\i }}k, Musialski, Auzinger, and Wimmer}}}
\bibcite{pix2pix}{{17}{2017}{{Isola et~al\unhbox \voidb@x \hbox {.}}}{{Isola, Zhu, Zhou, and Efros}}}
\bibcite{style-transfer}{{18}{2016}{{Johnson et~al\unhbox \voidb@x \hbox {.}}}{{Johnson, Alahi, and fei Li}}}
\bibcite{Wang:2018:DCP}{{19}{2018}{{Kai~Wang and Ritchie}}{{Kai~Wang and Ritchie}}}
\bibcite{Karras:2018:PGG}{{20}{2018}{{Karras et~al\unhbox \voidb@x \hbox {.}}}{{Karras, Aila, Laine, and Lehtinen}}}
\bibcite{Kelly:SIGA:2017}{{21}{2017}{{Kelly et~al\unhbox \voidb@x \hbox {.}}}{{Kelly, Femiani, Wonka, and Mitra}}}
\bibcite{vae}{{22}{2014}{{Kingma and Welling}}{{Kingma and Welling}}}
\bibcite{adam}{{23}{2015}{{Kingma and Ba}}{{Kingma and Ba}}}
\bibcite{krizhevsky2012imagenet}{{24}{2012}{{Krizhevsky et~al\unhbox \voidb@x \hbox {.}}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{vae_gan}{{25}{2016}{{Larsen et~al\unhbox \voidb@x \hbox {.}}}{{Larsen, S{\o }nderby, Larochelle, and Winther}}}
\bibcite{superresolution}{{26}{2017}{{Ledig et~al\unhbox \voidb@x \hbox {.}}}{{Ledig, Theis, Huszar, Caballero, Cunningham, Acosta, Aitken, Tejani, Totz, Wang, and Shi}}}
\bibcite{Lin:2011:SPR}{{27}{2011}{{Lin et~al\unhbox \voidb@x \hbox {.}}}{{Lin, Cohen-Or, Zhang, Liang, Sharf, Deussen, and Chen}}}
\bibcite{long2015fully}{{28}{2015}{{Long et~al\unhbox \voidb@x \hbox {.}}}{{Long, Shelhamer, and Darrell}}}
\bibcite{Martinovic:2013:BGL}{{29}{2013}{{Martinovic and Van~Gool}}{{Martinovic and Van~Gool}}}
\bibcite{Mathias:IJCV:2016}{{30}{2016}{{Mathias et~al\unhbox \voidb@x \hbox {.}}}{{Mathias, Martinovi\'{c}, and Gool}}}
\bibcite{Mueller:2006:PMB}{{31}{2006}{{Mueller et~al\unhbox \voidb@x \hbox {.}}}{{Mueller, Wonka, Haegler, Ulmer, and Van~Gool}}}
\bibcite{Nishida:2016:ISU:2897824.2925951}{{32}{2016}{{Nishida et~al\unhbox \voidb@x \hbox {.}}}{{Nishida, Garcia-Dorado, Aliaga, Benes, and Bousseau}}}
\bibcite{Peng:2016:CND}{{33}{2016}{{Peng et~al\unhbox \voidb@x \hbox {.}}}{{Peng, Yang, Bao, Fink, Yan, Wonka, and Mitra}}}
\bibcite{Ritchie:2015:CPM}{{34}{2015}{{Ritchie et~al\unhbox \voidb@x \hbox {.}}}{{Ritchie, Mildenhall, Goodman, and Hanrahan}}}
\bibcite{inception-GANs2}{{35}{2016}{{Salimans et~al\unhbox \voidb@x \hbox {.}}}{{Salimans, Goodfellow, Zaremba, Cheung, Radford, Chen, and Chen}}}
\bibcite{Schwarz:2015:APM}{{36}{2015}{{Schwarz and M{\"u}ller}}{{Schwarz and M{\"u}ller}}}
\bibcite{catGAN}{{37}{2016}{{Springenberg}}{{Springenberg}}}
\bibcite{Stolcke:1994:IPG}{{38}{1994}{{Stolcke and Omohundro}}{{Stolcke and Omohundro}}}
\bibcite{Talton:2011:MPM}{{39}{2011}{{Talton et~al\unhbox \voidb@x \hbox {.}}}{{Talton, Lou, Lesser, Duke, M{\v {e}}ch, and Koltun}}}
\bibcite{Talton:2012:LDP}{{40}{2012}{{Talton et~al\unhbox \voidb@x \hbox {.}}}{{Talton, Yang, Kumar, Lim, Goodman, and M{\v {e}}ch}}}
\bibcite{Teboul:PAMI:2013}{{41}{2013}{{Teboul et~al\unhbox \voidb@x \hbox {.}}}{{Teboul, Kokkinos, Simon, Koutsourakis, and Paragios}}}
\bibcite{tran2015learning}{{42}{2015}{{Tran et~al\unhbox \voidb@x \hbox {.}}}{{Tran, Bourdev, Fergus, Torresani, and Paluri}}}
\bibcite{face-recog}{{43}{2017}{{Tran et~al\unhbox \voidb@x \hbox {.}}}{{Tran, Yin, and Liu}}}
\bibcite{cmp_dataset}{{44}{2013}{{Tyle{\v c}ek and {\v S}{\' a}ra}}{{Tyle{\v c}ek and {\v S}{\' a}ra}}}
\bibcite{Vanegas:2012:IDU}{{45}{2012}{{Vanegas et~al\unhbox \voidb@x \hbox {.}}}{{Vanegas, Garcia-Dorado, Aliaga, Benes, and Waddell}}}
\bibcite{sceneGAN}{{46}{2017}{{Veeravasarapu et~al\unhbox \voidb@x \hbox {.}}}{{Veeravasarapu, Rothkopf, and Ramesh}}}
\bibcite{hand-pose}{{47}{2017}{{Wan et~al\unhbox \voidb@x \hbox {.}}}{{Wan, Probst, Van~Gool, and Yao}}}
\bibcite{improvGANfeature}{{48}{2017}{{Warde-Farley and Bengio}}{{Warde-Farley and Bengio}}}
\bibcite{Wonka:2003:IA}{{49}{2003}{{Wonka et~al\unhbox \voidb@x \hbox {.}}}{{Wonka, Wimmer, Sillion, and Ribarsky}}}
\bibcite{inpainting}{{50}{2016}{{Yeh et~al\unhbox \voidb@x \hbox {.}}}{{Yeh, Chen, Lim, Hasegawa{-}Johnson, and Do}}}
\bibcite{Yeh:2013:STP}{{51}{2013}{{Yeh et~al\unhbox \voidb@x \hbox {.}}}{{Yeh, Breeden, Yang, Fisher, and Hanrahan}}}
\bibcite{zhao2016energy}{{52}{2017}{{Zhao et~al\unhbox \voidb@x \hbox {.}}}{{Zhao, Mathieu, and LeCun}}}
\bibcite{manifold-manipulation}{{53}{2016}{{Zhu et~al\unhbox \voidb@x \hbox {.}}}{{Zhu, Kr{\"a}henb{\"u}hl, Shechtman, and Efros}}}
\bibcite{cycleGAN}{{54}{2017a}{{Zhu et~al\unhbox \voidb@x \hbox {.}}}{{Zhu, Park, Isola, and Efros}}}
\bibcite{zhu2017multimodal}{{55}{2017b}{{Zhu et~al\unhbox \voidb@x \hbox {.}}}{{Zhu, Zhang, Pathak, Darrell, Efros, Wang, and Shechtman}}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{4.185pt}
\newlabel{tocindent2}{10.34999pt}
\newlabel{tocindent3}{0pt}
\@writefile{toc}{\contentsline {section}{Acknowledgments}{14}{section*.35}}
\@writefile{toc}{\contentsline {section}{References}{14}{section*.37}}
\newlabel{TotPages}{{14}{14}{}{page.14}{}}
